{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from bigdl.dataset import mnist\n",
    "from bigdl.util.common import init_engine, Sample\n",
    "\n",
    "from bigdl.nn.layer import Linear, SpatialMaxPooling, \\\n",
    "    SpatialConvolution, ReLU, Sequential, Reshape, LogSoftMax\n",
    "    \n",
    "from bigdl.optim.optimizer import Optimizer, Adam, MaxEpoch, EveryEpoch, Top1Accuracy, \\\n",
    "    TrainSummary, ValidationSummary, SeveralIteration, SGD\n",
    "\n",
    "from bigdl.nn.criterion import ClassNLLCriterion, CrossEntropyCriterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x103f6dc90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "traffic_dir = 'traffic_signs_data'\n",
    "\n",
    "with gzip.open(traffic_dir + '/train2.p.gz', mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with gzip.open(traffic_dir + '/test2.p.gz', mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['images'], train['labels']\n",
    "X_test, y_test = test['images'], test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39209, 32, 32, 3), (39209,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLASS_COUNT = len(np.unique(y_train))\n",
    "assert len(np.unique(y_test)) == CLASS_COUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize data.\n",
    "Use Min/Max normalization to improve convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(image_data, labels, min_x=None, max_x=None):\n",
    "    min_x = np.min(image_data) if min_x is None else min_x\n",
    "    max_x = np.max(image_data) if max_x is None else max_x\n",
    "    delta = max_x - min_x\n",
    "    a, b = 0.1, 0.9\n",
    "#     result = a + (image_data - min_x) * (b - a) / (max_x - min_x)\n",
    "#     return result, min_x, max_x\n",
    "\n",
    "    rdd_images = sc.parallelize(image_data)\n",
    "    rdd_labels = sc.parallelize(labels)\n",
    "\n",
    "    rdd_sample = rdd_images \\\n",
    "        .zip(rdd_labels) \\\n",
    "        .map(lambda (features, label): \\\n",
    "             Sample.from_ndarray((features - min_x) * (b - a) / delta, label + 1))\n",
    "    return rdd_sample, min_x, max_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_norm, min_x, max_x = normalize(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: apply the same Min/Max values from the training set to the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_norm, _, _ = normalize(X_test, y_test, min_x, max_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# samples = X_test_norm.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# samples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bigdl.nn.initialization_method import Xavier, Zeros, RandomNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LeNet(class_num):\n",
    "    mu = 0.01\n",
    "    sigma = 0.1\n",
    "    w_init = RandomNormal(mu, sigma)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Reshape([3, 32, 32]))\n",
    "    \n",
    "    model.add(SpatialConvolution(3, 6, 5, 5).set_init_method(w_init, Zeros()).set_name('conv1'))\n",
    "    model.add(ReLU())\n",
    "    \n",
    "    model.add(SpatialMaxPooling(2, 2, 2, 2).set_name('pool1'))\n",
    "    \n",
    "    model.add(SpatialConvolution(6, 16, 5, 5).set_init_method(w_init, Zeros()).set_name('conv2'))\n",
    "    model.add(ReLU())\n",
    "    \n",
    "    model.add(SpatialMaxPooling(2, 2, 2, 2).set_name('pool2'))\n",
    "    \n",
    "    model.add(Reshape([400]))\n",
    "    \n",
    "    model.add(Linear(400, 120).set_init_method(w_init, Zeros()).set_name('fc1'))\n",
    "    model.add(ReLU())\n",
    "\n",
    "#     model.add(Linear(200, 120).set_name('fc1_5'))\n",
    "#     model.add(ReLU())\n",
    "\n",
    "    model.add(Linear(120, 84).set_init_method(w_init, Zeros()).set_name('fc2'))\n",
    "    model.add(ReLU())\n",
    "\n",
    "    model.add(Linear(84, class_num).set_init_method(w_init, Zeros()).set_name('logits'))\n",
    "#     model.add(LogSoftMax())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createRandomNormal\n",
      "creating: createSequential\n",
      "creating: createReshape\n",
      "creating: createSpatialConvolution\n",
      "creating: createZeros\n",
      "creating: createReLU\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createSpatialConvolution\n",
      "creating: createZeros\n",
      "creating: createReLU\n",
      "creating: createSpatialMaxPooling\n",
      "creating: createReshape\n",
      "creating: createLinear\n",
      "creating: createZeros\n",
      "creating: createReLU\n",
      "creating: createLinear\n",
      "creating: createZeros\n",
      "creating: createReLU\n",
      "creating: createLinear\n",
      "creating: createZeros\n"
     ]
    }
   ],
   "source": [
    "lenet_model = LeNet(CLASS_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createCrossEntropyCriterion\n",
      "creating: createAdam\n",
      "creating: createMaxEpoch\n",
      "creating: createOptimizer\n"
     ]
    }
   ],
   "source": [
    "optimizer = Optimizer(model=lenet_model, training_rdd=X_train_norm,\n",
    "    criterion=CrossEntropyCriterion(),\n",
    "#                       criterion=ClassNLLCriterion(),\n",
    "    optim_method=Adam(learningrate=1e-3, beta1=0.9, beta2=0.999, epsilon=1e-8),\n",
    "#                       optim_method=SGD(nesterov=True, \n",
    "#                                        momentum=0.9, \n",
    "#                                        dampening=0.0,\n",
    "#                                        learningrate=1e-4),\n",
    "    end_trigger=MaxEpoch(30),\n",
    "    batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createEveryEpoch\n",
      "creating: createTop1Accuracy\n"
     ]
    }
   ],
   "source": [
    "# Set the validation logic\n",
    "optimizer.set_validation(batch_size=64, \n",
    "                         val_rdd=X_test_norm,\n",
    "                         trigger=EveryEpoch(),\n",
    "                         val_method=[Top1Accuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createTrainSummary\n",
      "creating: createSeveralIteration\n",
      "creating: createValidationSummary\n",
      "('saving logs to ', 'lenet5-')\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "app_name= 'lenet5-' # + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# Remove old files:\n",
    "base_path = os.path.join('/tmp/bigdl_summaries', app_name)\n",
    "try:\n",
    "    shutil.rmtree(base_path)\n",
    "    shutil.rmtree('/private' + base_path)  # On Mac\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "train_summary = TrainSummary(log_dir='/tmp/bigdl_summaries', app_name=app_name)\n",
    "train_summary.set_summary_trigger(\"Parameters\", SeveralIteration(1))\n",
    "val_summary = ValidationSummary(log_dir='/tmp/bigdl_summaries', app_name=app_name)\n",
    "optimizer.set_train_summary(train_summary)\n",
    "optimizer.set_val_summary(val_summary)\n",
    "\n",
    "    \n",
    "print(\"saving logs to \", app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "CPU times: user 112 ms, sys: 56.9 ms, total: 169 ms\n",
      "Wall time: 26min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trained_model = optimizer.optimize()\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
