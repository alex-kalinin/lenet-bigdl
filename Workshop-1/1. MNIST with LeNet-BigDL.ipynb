{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Handwritten Digits with LeNet/BigDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from bigdl.dataset import mnist\n",
    "from bigdl.util.common import init_engine, Sample\n",
    "\n",
    "from bigdl.nn.layer import Linear, SpatialMaxPooling, \\\n",
    "    SpatialConvolution, ReLU, Sequential, Reshape, LogSoftMax\n",
    "    \n",
    "from bigdl.optim.optimizer import Optimizer, Adam, MaxEpoch, EveryEpoch, Top1Accuracy, \\\n",
    "    TrainSummary, ValidationSummary, SeveralIteration, SGD\n",
    "\n",
    "from bigdl.nn.criterion import ClassNLLCriterion, CrossEntropyCriterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_PATH = 'MNIST_data'\n",
    "(train_images, train_labels) = mnist.read_data_sets(MNIST_PATH, \"train\")\n",
    "(test_images, test_labels) = mnist.read_data_sets(MNIST_PATH, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape, train_labels.shape, test_images.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=1000, linewidth=10000)\n",
    "\n",
    "def display(X, y, n):\n",
    "    pic = X[n].reshape(28, 28)\n",
    "    plt.imshow(pic, cmap='gray')\n",
    "    with pd.option_context(\"display.max_columns\", 1000):\n",
    "        print(n)\n",
    "        print(y[n])\n",
    "        print(pic)\n",
    "    \n",
    "n = np.random.randint(0, train_images.shape[0])\n",
    "display(train_images, train_labels, 3243)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_COUNT = len(np.unique(train_labels))\n",
    "assert len(np.unique(train_labels)) == CLASS_COUNT\n",
    "CLASS_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize data\n",
    "\n",
    "Data normalization helps the numerical algorithms to converge faster (or at all).\n",
    "Our data is in range [0, 255]; we will normalize it to be in the range [0.1, 0.9]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image_data, labels, min_x=None, max_x=None):\n",
    "    min_x = np.min(image_data) if min_x is None else min_x\n",
    "    max_x = np.max(image_data) if max_x is None else max_x\n",
    "    delta = max_x - min_x\n",
    "    a, b = 0.1, 0.9\n",
    "\n",
    "    rdd_images = sc.parallelize(image_data)\n",
    "    rdd_labels = sc.parallelize(labels)\n",
    "\n",
    "    rdd_sample = rdd_images \\\n",
    "        .zip(rdd_labels) \\\n",
    "        .map(lambda features_labels: \\\n",
    "             Sample.from_ndarray((features_labels[0] - min_x) * (b - a) / delta, features_labels[1] + 1))\n",
    "    return rdd_sample, min_x, max_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize data.\n",
    "Use Min/Max normalization to improve convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm, min_x, max_x = normalize(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: apply the same Min/Max values from the training set to the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_norm, _, _ = normalize(test_images, test_labels, min_x, max_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = X_test_norm.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet(class_num):\n",
    "    model = Sequential()\n",
    "    model.add(Reshape([1, 28, 28]))\n",
    "    \n",
    "    model.add(SpatialConvolution(1, 6, 5, 5).set_name('conv1'))\n",
    "    model.add(ReLU())\n",
    "    \n",
    "    model.add(SpatialMaxPooling(2, 2, 2, 2).set_name('pool1'))\n",
    "    \n",
    "    model.add(SpatialConvolution(6, 16, 5, 5).set_name('conv2'))\n",
    "    model.add(ReLU())\n",
    "    \n",
    "    model.add(SpatialMaxPooling(2, 2, 2, 2).set_name('pool2'))\n",
    "    \n",
    "    model.add(Reshape([16 * 4 * 4]))\n",
    "    \n",
    "    model.add(Linear(16 * 4 * 4, 84).set_name('fc1'))\n",
    "    model.add(ReLU())\n",
    "\n",
    "#     model.add(Linear(120, 84).set_name('fc2'))\n",
    "#     model.add(ReLU())\n",
    "\n",
    "    model.add(Linear(84, class_num).set_name('score'))\n",
    "    model.add(LogSoftMax())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_model = LeNet(CLASS_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Optimizer(model=lenet_model, training_rdd=X_train_norm,\n",
    "                      criterion=ClassNLLCriterion(),\n",
    "                      optim_method=SGD(nesterov=True, momentum=0.9, dampening=0.0),\n",
    "                      end_trigger=MaxEpoch(EPOCHS),\n",
    "                      batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the validation logic\n",
    "optimizer.set_validation(batch_size=128, val_rdd=X_test_norm,\n",
    "                         trigger=EveryEpoch(),\n",
    "                         val_method=[Top1Accuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "tensorboard_dir=os.environ.get('TENSORBOARD_DIR', '/tmp/tensorboard-logs')\n",
    "print(\"TENSORBOARD_DIR : \", tensorboard_dir)\n",
    "\n",
    "app_name='lenet5' #+dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "base_path = os.path.abspath(os.path.join(tensorboard_dir, app_name))\n",
    "\n",
    "# clean old logs\n",
    "try:\n",
    "    print (\"Cleaning : \", base_path)\n",
    "    shutil.rmtree(base_path)\n",
    "except OSError as e:\n",
    "    print (e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_summary = TrainSummary(log_dir=tensorboard_dir, app_name=app_name)\n",
    "train_summary.set_summary_trigger(\"Parameters\", SeveralIteration(10)) \n",
    "val_summary = ValidationSummary(log_dir=tensorboard_dir, app_name=app_name)\n",
    "optimizer.set_train_summary(train_summary)\n",
    "optimizer.set_val_summary(val_summary)\n",
    "print(\"saving logs to \", app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trained_model = optimizer.optimize()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "predictions = trained_model.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predictions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_classes = [np.argmax(p) for p in preds]\n",
    "pred_classes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def display_letter_predict(pred_classes):\n",
    "    N = test_images.shape[0]\n",
    "    i = random.randint(0, N)\n",
    "    image = test_images[i]\n",
    "    pred = pred_classes[i]\n",
    "    print('Prediction:', pred)\n",
    "    display(test_images, test_labels, i)\n",
    "\n",
    "display_letter_predict(pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem:\n",
    "\n",
    "Find several example from the test prediction where we incorrectly predicted the letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
